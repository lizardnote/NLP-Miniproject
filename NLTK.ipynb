{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK1iupVtVj8qCfLEHga4RI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizardnote/NLP_practice/blob/main/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK\n"
      ],
      "metadata": {
        "id": "yXJcIRIE7JCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "fKgMj6I-T_1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 41
        },
        "outputId": "add5ee99-4398-4754-97e9-d4206916ca74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a1c51259-9e27-484c-9e40-d0227704d33c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a1c51259-9e27-484c-9e40-d0227704d33c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1. 필수 라이브러리 설치 (필요할 경우만)\n",
        "# !pip install lightgbm transformers konlpy etc...\n",
        "\n",
        "# ✅ 2. nltk 리소스 다운로드 (한 번은 필수)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# ✅ 3. 사용할 토크나이저 설정\n",
        "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer\n",
        "\n",
        "# ✅ 문장 분리기\n",
        "sent_tokenizer = PunktSentenceTokenizer()\n",
        "\n",
        "# ✅ 단어 분리기: 정규표현식 기반 (단어 기준으로 잘라줌)\n",
        "word_tokenizer = RegexpTokenizer(r'\\w+')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9YNETceTNt4",
        "outputId": "92caf600-958c-4def-c035-7d7a1e3faa26"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Analytics\n",
        "- 테스트 사전 준비작업(전처리)\n",
        "- 피쳐 벡터화/추출\n",
        "- ML모델 수립 및 학습/예측 평가"
      ],
      "metadata": {
        "id": "VGuK90jpvGKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#연습\n",
        "text = \"카드를 재발급 받고 싶어요. 한도 조회는 어디서 하나요?\"\n",
        "\n",
        "# 문장 분리\n",
        "print(\"문장 분리:\", sent_tokenizer.tokenize(text))\n",
        "\n",
        "# 단어 분리 (단순하지만 안정적!)\n",
        "print(\"단어 분리:\", word_tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Kcs5DeSvX4",
        "outputId": "4401db44-94aa-425a-9be0-200b4f058b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 분리: ['카드를 재발급 받고 싶어요.', '한도 조회는 어디서 하나요?']\n",
            "단어 분리: ['카드를', '재발급', '받고', '싶어요', '한도', '조회는', '어디서', '하나요']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "EHRhH4M9SwBg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 전처리\n",
        "- 클렌징 : html 태그 같은 특정 기호, 불필요한 문자 제거\n",
        "- 토큰화 : 주로 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리, 정규표현식으로도 토큰화 가능\n",
        "- 필터링/스톱 워드 제거/철자 수정\n",
        "- Stemming\n",
        "- Lemmatization"
      ],
      "metadata": {
        "id": "dt8WXz5yvnOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06RHveLDveMl",
        "outputId": "0d0c9706-5329-4f59-8cca-f77073c697c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPIoPltxM8N",
        "outputId": "a69e16f8-9a82-4018-acaf-074dc2a9962b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sent_tokenize\n",
        "- 각 문장으로 구성된 객체 list 반환\n",
        "- 문장이 가지는 시맨틱적 의미가 중요한 요소로 사용될 때 문장 토큰화 필요"
      ],
      "metadata": {
        "id": "lKCeuiZRxbNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "              You can see it out your window or on your television \\\n",
        "              You feel it when you go to work, or go to chuch or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)"
      ],
      "metadata": {
        "id": "yA1QaIWGwf3R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D00UN7vxw7X_",
        "outputId": "d2631eb1-f9b6-4783-9dcd-bc4a2c716861"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 2\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television               You feel it when you go to work, or go to chuch or pay your taxes.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word_tokenize\n"
      ],
      "metadata": {
        "id": "6S2QWOYfxiX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The matrix is everywhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_64pkZa1xU8V",
        "outputId": "11513f73-ae73-45d2-d9e0-de8a00106869"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "#여러개 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
        "\n",
        "def tokenize_text(text):\n",
        "\n",
        "  sentences = sent_tokenize(text)                        #문장별 분리 토큰\n",
        "  word_tokens = [word_tokenize(i) for i in sentences]    #분리된 문장별 단어 토큰화\n",
        "  return word_tokens\n",
        "\n",
        "\n",
        "#여러 문장에 대한 문장별 단어 토큰화\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LRfii6txs5L",
        "outputId": "e9c87807-1ccb-4680-8423-f3a47046d916"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 2\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', 'You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'chuch', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Word 제거하기"
      ],
      "metadata": {
        "id": "gNLlLnbJx6jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "RxsvWc5_yoNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b3fb04-b982-47a5-d0f8-33039a684d10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('영어 stop words 갯수:', len(nltk.corpus.stopwords.words('english')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgPsidxJyGnV",
        "outputId": "f3314331-7ba1-4b9c-d007-2be1edd9cae6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 갯수: 198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldf4ZMH8yJ5I",
        "outputId": "dcadce95-7ac2-4aad-f45b-06dcb29d91bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "#위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 stop word 제거\n",
        "for sentence in word_tokens:\n",
        "  filtered_words = []\n",
        "  #개별 문장별로 토큰화된 문장 list에서 stop word 제거\n",
        "  for word in sentence:\n",
        "    word = word.lower()  #소문자 변환\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "\n",
        "\n",
        "  all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SbBlzLyyRDw",
        "outputId": "c34e7f16-32ba-4e38-929b-9dbdbce4ab82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', 'feel', 'go', 'work', ',', 'go', 'chuch', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and Lemmatization\n",
        ": 단어 원형 찾기\n",
        "  - stemming : 원래 단어에서 일부 철자가 훼손된 언근 단어를 추출하는 방식\n",
        "  - lemmatization : 품사가 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 찾는 방식\n",
        "  - Porter, lancaster, snowball stemmer 등 제공\n",
        "  - WordNetlemmatizer 제공"
      ],
      "metadata": {
        "id": "xYk7DRxtzTt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQhtyKG4zEtt",
        "outputId": "b5fbffc5-6ea3-4298-dd04-0960259cacbc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_7vBUsnAKtJ",
        "outputId": "eb366a13-e7f0-4a83-fa9e-eb332ff32312"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amuse amuse amuse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemma.lemmatize('happiest', 'a'), lemma.lemmatize('happier', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEWc2WdBARfg",
        "outputId": "f32f89b1-927b-4c05-8f27-13ae7bb6ea4f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of words : Bow\n",
        "---\n",
        "- bow모델은 쉽고 빠른 구축이 가능함 / 발생 횟수 기반\n",
        "- 단, 문맥 의미 반영이 부족하고 희소 행렬 문제로 NLP연구에 제약이 있음\n",
        "\n",
        "### Bow 피쳐 벡터화\n",
        "- 카운트 기반의 벡터화 : count를 부여하는 방식\n",
        "- TF-IDF 기반의 벡터화 : count 하면서 가중치(개별 문서) / 패널티를 주며 벡터화 함(모든 문서에 자주 등장하는 단어는 패널티를 주는 식)\n",
        "\n",
        "\n",
        "### CountVectorizer , TfidfVectorizer\n",
        "1.   문자들 소문자로 변경(전처리 작업)\n",
        "2.   default: 단어 기준  / n_gram_range 반영해 각 단어 토큰화 가능\n",
        "3.   텍스트 정규화(stop_word 필터링 / 어근 변환 - stemming, lemmatization)\n",
        "4.   파라미터로 토큰화된 단어 피쳐로 추출, 빈도수 벡터화\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xxPXnykaAtL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 희소 행렬 : COO ,CSR\n",
        "- 희소행렬을 물리적으로 적은 메모리 공간을 차지하게 변환하는 방식\n",
        "\n",
        "#### COO (coordinate 좌표)\n",
        "- 0이 아닌 데이터만 별도의 데이터 배열에 저장\n",
        "- 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장\n",
        "\n",
        "#### CSR (compressed sparse row)\n",
        "- 행 위치 배열의 고유한 값의 시작 위치만 표기해 반복을 제거\n",
        "- 위치의 위치를 표시하는 배열을 생성"
      ],
      "metadata": {
        "id": "g0rpOwBdSMgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COO\n",
        "import numpy as np\n",
        "\n",
        "dense = np.array([ [3,0,1] , [0,2,0]])\n",
        "print(dense)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLScRxv5Aiw7",
        "outputId": "bdb93199-e759-4712-85de-1b9aee6d76d5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3 0 1]\n",
            " [0 2 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "# 0이 아닌 데이터 추출\n",
        "data = np.array([3,1,2])\n",
        "\n",
        "# 행 위치와 열 위치를 각각 배열로 생성\n",
        "row_pos = np.array([0, 0, 1])\n",
        "col_pos = np.array([0, 2, 1])\n",
        "\n",
        "# sparse 패키지의 coo_matrix로 coo형식 희소 행렬 생성\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
        "print(sparse_coo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-DGsIo4Q8AV",
        "outputId": "0219eab3-b623-4268-9135-bef8a32da13b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<COOrdinate sparse matrix of dtype 'int64'\n",
            "\twith 3 stored elements and shape (2, 3)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t3\n",
            "  (0, 2)\t1\n",
            "  (1, 1)\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_coo.toarray()  #밀집행렬로 다실 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU5VnFoeRnpb",
        "outputId": "6cf5750d-00ce-41db-a656-f6c9b4cb67a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CSR\n",
        "from scipy import sparse\n",
        "\n",
        "dense2 = np.array([[0,0,1,0,0,5],\n",
        "             [1,4,0,3,2,5],\n",
        "             [0,6,0,3,0,0],\n",
        "             [2,0,0,0,0,0],\n",
        "             [0,0,0,7,0,8],\n",
        "             [1,0,0,0,0,0]])\n",
        "\n",
        "# 0이 아닌 데이터\n",
        "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
        "\n",
        "#행 위치와 열 위치\n",
        "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
        "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
        "\n",
        "# coo 형식으로 변환\n",
        "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
        "\n",
        "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
        "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
        "\n",
        "# CSR 형식으로 변환\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print('COO 변환된 데이터가 제대로 되었는지 확인')\n",
        "print(sparse_coo.toarray())\n",
        "print('CSR 변환된 데이터가 제대로 되었는지 확인')\n",
        "print(sparse_csr.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZrnn0ZjRuDe",
        "outputId": "a1a7cf22-6e04-4df7-f08b-07b7d84cbfaa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COO 변환된 데이터가 제대로 되었는지 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "CSR 변환된 데이터가 제대로 되었는지 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense3 = np.array([[0,0,1,0,0,5],\n",
        "                   [1,4,0,3,2,5],\n",
        "                   [0,6,0,3,0,0],\n",
        "                   [2,0,0,0,0,0],\n",
        "                   [0,0,0,7,0,8],\n",
        "                   [1,0,0,0,0,0]])\n",
        "\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)"
      ],
      "metadata": {
        "id": "pHgOSM9_UCWR"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}